{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2 - Flowers Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "from torch.nn import MaxPool2d\n",
    "from torch.nn import AvgPool2d\n",
    "from torch.autograd import Variable\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import ImageFolder\n",
    "use_cuda = torch.cuda.is_available()\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "testlosses=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4323 4323\n"
     ]
    }
   ],
   "source": [
    "#Loading the dataset numpy files\n",
    "data=np.load(\"flower_imgs.npy\")\n",
    "label=np.load(\"flower_labels.npy\")\n",
    "print(len(data),len(label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Finding mean and standard deviation of all images in the dataset\n",
    "img_mean = np.mean(np.swapaxes(data/255.0,0,1).reshape(3, -1), 1)\n",
    "img_std = np.std(np.swapaxes(data/255.0,0,1).reshape(3, -1), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Transformations on the dataset\n",
    "#Normalise on the calculated mean and standard deviation\n",
    "normalize = transforms.Normalize(mean=list(img_mean), std=list(img_std))\n",
    "#Other transformations: Crop Image and horizontally flip\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.ToPILImage(), transforms.RandomCrop(32, padding=4), transforms.RandomHorizontalFlip(), normalize])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3674, 32, 32, 3) (3674,) (649, 32, 32, 3) (649,)\n"
     ]
    }
   ],
   "source": [
    "#Splitting the dataset into trainign and testing: The split is the default split of: 85% (training) - 15% (testing)\n",
    "x_train, x_test, y_train, y_test = train_test_split(data, label, test_size = 0.15)\n",
    "print(np.array(x_train).shape, np.array(y_train).shape, np.array(x_test).shape, np.array(y_test).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loads the images using loader class\n",
    "#Abstract clss representing a dataset\n",
    "class ImgLoader(Dataset):\n",
    "    def __init__(self, x, y,iscuda=False, transform=None):\n",
    "        self.x = np.array(x)\n",
    "        self.y = np.array(y)\n",
    "        self.transform=transform\n",
    "        self.cuda = iscuda\n",
    "\n",
    "#supports indexing\n",
    "    def __getitem__(self, index):\n",
    "        x_val = self.x[index]\n",
    "        x_val = torch.from_numpy(x_val).permute(2, 1, 0)\n",
    "        y_val = torch.from_numpy(np.array([self.y[index]]))\n",
    "        return x_val, y_val\n",
    "\n",
    "#returns size of dataset\n",
    "    def __len__(self):\n",
    "        return len(self.x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checks for whether cuda is available so as to run on GPU or not\n",
    "use_cuda = torch.cuda.is_available()\n",
    "#img_loadertrain has the training images; on which transformations have been performed\n",
    "img_loadertrain = ImgLoader(x_train, y_train, use_cuda, transform=transform)\n",
    "#Training data loaded into the Network having batch size = 100\n",
    "trainloader = DataLoader(img_loadertrain, batch_size=100, shuffle=True, num_workers=4)\n",
    "\n",
    "#img_loadertest has the testing images; on which transformations have been performed\n",
    "img_loadertest = ImgLoader(x_test, y_test, use_cuda, transform=transforms.Compose([transforms.ToTensor(), transforms.ToPILImage(), transforms.ToTensor(),normalize]))\n",
    "#Testing data loaded into the Network having batch size = 100\n",
    "testloader = DataLoader(img_loadertest, batch_size=100, shuffle=True, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Network model 1 which stays stagnant after a point\n",
    "# class ConvClassifier(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(ConvClassifier, self).__init__()\n",
    "#         self.conv1 = nn.Conv2d(3, 64, kernel_size=3, padding=1)\n",
    "#         self.bn1 = nn.BatchNorm2d(64)\n",
    "#         self.conv2 = nn.Conv2d(64, 64, kernel_size=3, padding=1, stride=1)\n",
    "#         self.bn2 = nn.BatchNorm2d(64)\n",
    "#         self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1, stride=1)\n",
    "#         self.bn3 = nn.BatchNorm2d(128)\n",
    "\n",
    "#         self.conv4 = nn.Conv2d(128, 64, kernel_size=3, padding=1, stride=1)\n",
    "#         self.bn4 = nn.BatchNorm2d(64)\n",
    "\n",
    "#         self.conv5 = nn.Conv2d(64, 64, kernel_size=5, padding=1, stride=2)\n",
    "#         self.bn5 = nn.BatchNorm2d(64)\n",
    "#         self.fc1 = nn.Linear(64*15*15, 100)\n",
    "#         self.fc2 = nn.Linear(100, 5)\n",
    "\n",
    "        \n",
    "#     def forward(self, x):\n",
    "#         self.conv_l1 = self.conv1(x)\n",
    "#         self.conv_l1 = self.bn1(self.conv_l1)\n",
    "        \n",
    "#         self.conv_l2 = F.relu(self.conv2(self.conv_l1))\n",
    "#         self.conv_l2 = self.bn2(self.conv_l2)\n",
    "        \n",
    "#         self.conv_l3 = F.relu(self.conv3(self.conv_l2))\n",
    "#         self.conv_l3 = self.bn3(self.conv_l3)\n",
    "\n",
    "#         self.conv_l4 = F.relu(self.conv4(self.conv_l3))\n",
    "#         self.conv_l4 = self.bn4(self.conv_l4)\n",
    "\n",
    "#         self.conv_l5 = F.relu(self.conv5(self.conv_l4))\n",
    "#         self.conv_l5 = self.bn5(self.conv_l5)\n",
    "\n",
    "#         self.fc_l1 = self.conv_l5.view(-1, 64 * 15 * 15)\n",
    "#         self.fc_l1 = F.relu(self.fc1(self.fc_l1))\n",
    "#         self.fc_l2 = self.fc2(self.fc_l1) \n",
    "        \n",
    "#         return F.log_softmax(self.fc_l2)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Network model 2\n",
    "cfg = {\n",
    "    'VGG11': [64, 'M', 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n",
    "    'VGG13': [64, 64, 'M', 128, 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n",
    "    'VGG16': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 'M', 512, 512, 512, 'M', 512, 512, 512, 'M'],\n",
    "    'VGG19': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 256, 'M', 512, 512, 512, 512, 'M', 512, 512, 512, 512, 'M'],\n",
    "}\n",
    "\n",
    "\n",
    "class VGG(nn.Module):\n",
    "    def __init__(self, vgg_name):\n",
    "        super(VGG, self).__init__()\n",
    "        self.features = self._make_layers(cfg[vgg_name])\n",
    "        self.classifier = nn.Linear(512, 5)\n",
    "       \n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.features(x)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.classifier(out)\n",
    "        out = F.softmax(out, dim=1)\n",
    "        return out\n",
    "\n",
    "    def _make_layers(self, cfg):\n",
    "        layers = []\n",
    "        in_channels = 3\n",
    "        for x in cfg:\n",
    "            if x == 'M':\n",
    "                layers += [nn.MaxPool2d(kernel_size=2, stride=2)]\n",
    "            else:\n",
    "                layers += [nn.Conv2d(in_channels, x, kernel_size=3, padding=1),\n",
    "                           nn.BatchNorm2d(x),\n",
    "                           nn.ReLU(inplace=True)]\n",
    "                in_channels = x\n",
    "        layers += [nn.AvgPool2d(kernel_size=1, stride=1)]\n",
    "        return nn.Sequential(*layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Network model 3\n",
    "# class ConvClassifier(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(ConvClassifier, self).__init__()\n",
    "#         self.conv1 = nn.Conv2d(3, 64, kernel_size=3, padding=1)\n",
    "#         self.bn1 = nn.BatchNorm2d(64)\n",
    "#         self.conv2 = nn.Conv2d(64, 64, kernel_size=3, padding=1, stride=1)\n",
    "#         self.bn2 = nn.BatchNorm2d(64)\n",
    "         \n",
    "#         self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "    \n",
    "#         self.conv4 = nn.Conv2d(64, 128, kernel_size=3, padding=1, stride=1)\n",
    "#         self.bn4 = nn.BatchNorm2d(128)\n",
    "\n",
    "#         self.conv5 = nn.Conv2d(128, 128, kernel_size=3, padding=1, stride=1)\n",
    "#         self.bn5 = nn.BatchNorm2d(128)\n",
    "\n",
    "#         self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "#         self.conv7 = nn.Conv2d(128, 256, kernel_size=3, padding=1, stride=1)\n",
    "#         self.bn7 = nn.BatchNorm2d(256)\n",
    "\n",
    "#         self.conv8 = nn.Conv2d(256, 256, kernel_size=3, padding=1, stride=1)\n",
    "#         self.bn8 = nn.BatchNorm2d(256)\n",
    "\n",
    "#         self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "    \n",
    "#         self.conv10 = nn.Conv2d(256, 512, kernel_size=3, padding=1, stride=1)\n",
    "#         self.bn10 = nn.BatchNorm2d(512)\n",
    "\n",
    "#         self.conv11 = nn.Conv2d(512, 512, kernel_size=3, padding=1, stride=1)\n",
    "#         self.bn11 = nn.BatchNorm2d(512)\n",
    "\n",
    "#         self.pool4 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "    \n",
    "#         self.conv13 = nn.Conv2d(512, 512, kernel_size=3, padding=1, stride=1)\n",
    "#         self.bn13 = nn.BatchNorm2d(512)\n",
    "\n",
    "#         self.conv14 = nn.Conv2d(512, 512, kernel_size=3, padding=1, stride=1)\n",
    "#         self.bn14 = nn.BatchNorm2d(512)\n",
    "\n",
    "#         self.pool5 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "    \n",
    "#         self.pool6 = nn.AvgPool2d(kernel_size=1, stride=1)\n",
    "\n",
    "#         self.fc1 = nn.Linear(512, 5)\n",
    "        \n",
    "#     def forward(self, x):\n",
    "#         self.conv_l1 = self.conv1(x)\n",
    "#         self.conv_l1 = self.bn1(self.conv_l1)\n",
    "        \n",
    "#         self.conv_l2 = F.relu(self.conv2(self.conv_l1))\n",
    "#         self.conv_l2 = self.bn2(self.conv_l2)\n",
    "        \n",
    "#         self.conv_l3 = self.pool1(self.conv_l2)\n",
    "        \n",
    "#         self.conv_l4 = F.relu(self.conv4(self.conv_l3))\n",
    "#         self.conv_l4 = self.bn4(self.conv_l4)\n",
    "\n",
    "#         self.conv_l5 = F.relu(self.conv5(self.conv_l4))\n",
    "#         self.conv_l5 = self.bn5(self.conv_l5)\n",
    "        \n",
    "#         self.conv_l6 = self.pool2(self.conv_l5)\n",
    "\n",
    "#         self.conv_l7 = F.relu(self.conv7(self.conv_l6))\n",
    "#         self.conv_l7 = self.bn7(self.conv_l7)        \n",
    "        \n",
    "#         self.conv_l8 = F.relu(self.conv8(self.conv_l7))\n",
    "#         self.conv_l8 = self.bn7(self.conv_l8)\n",
    "        \n",
    "#         self.conv_l9 = self.pool3(self.conv_l8)\n",
    "        \n",
    "#         self.conv_l10 = F.relu(self.conv10(self.conv_l9))\n",
    "#         self.conv_l10 = self.bn10(self.conv_l10)        \n",
    "        \n",
    "#         self.conv_l11 = F.relu(self.conv11(self.conv_l10))\n",
    "#         self.conv_l11 = self.bn11(self.conv_l11)\n",
    "        \n",
    "#         self.conv_l12 = self.pool4(self.conv_l11)\n",
    "        \n",
    "#         self.conv_l13 = F.relu(self.conv13(self.conv_l12))\n",
    "#         self.conv_l13 = self.bn13(self.conv_l13)        \n",
    "        \n",
    "#         self.conv_l14 = F.relu(self.conv14(self.conv_l13))\n",
    "#         self.conv_l14 = self.bn14(self.conv_l14)\n",
    "        \n",
    "#         self.conv_l15 = self.pool5(self.conv_l14)\n",
    "\n",
    "#         self.conv_l16 = self.pool6(self.conv_l15)\n",
    "        \n",
    "#         self.fc_l1 = self.conv_l16.view(x.size(0), -1)\n",
    "        \n",
    "#         return F.softmax(self.fc_l1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Eval function called to calculate the accuracy of model\n",
    "accuracy = 0\n",
    "def eval(model, testloader):\n",
    "    correct = 0\n",
    "    for x,y in testloader:\n",
    "        #x = torch.from_numpy(np.array([x])).permute(0, 3, 1, 2).type(torch.float32)\n",
    "        x=x.type(torch.FloatTensor)\n",
    "        y=y.type(torch.LongTensor)\n",
    "        if use_cuda:\n",
    "            x = x.cuda()\n",
    "            y = y.cuda()\n",
    "        out = model(Variable(x))\n",
    "        #for multiclass classification, take class with maximum probability when got out of softmax output layer\n",
    "        pred = out.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
    "        correct += pred.eq(y.view_as(pred)).sum().item()\n",
    "    accuracy = correct * 1.0 / len(testloader)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predi(model, x_train, y_train):\n",
    "    i=0\n",
    "    for (x,y) in zip(x_train,y_train):\n",
    "        x = torch.from_numpy(np.array([x])).type(torch.float32)\n",
    "        if use_cuda:\n",
    "            x = x.cuda()\n",
    "        out = model(Variable(x))\n",
    "        #for binary classification, if predicted probability is more than 0.5, label as 1, else 0\n",
    "        pred = out.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
    "        label = pred.eq(y.view_as(pred)).sum().item()\n",
    "        if(y != label):\n",
    "            print(\"index prediction actual\")\n",
    "            print(i+1, label, y)\n",
    "        i = i+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred(net, x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaltestloss function called to calculate the loss of testing data on the trained model\n",
    "testlosses = []\n",
    "def evaltestloss(model, testloader):\n",
    "    net.train(False)\n",
    "    for i, (x, y) in enumerate(testloader):\n",
    "        x = x.type(torch.FloatTensor)\n",
    "        if use_cuda:\n",
    "            x = x.cuda()\n",
    "            y = y.cuda()\n",
    "        #reinitialise the optimiser gradient to 0\n",
    "        optimizer.zero_grad()\n",
    "        #input to the network is training data\n",
    "        x = Variable(x)\n",
    "        out = model(x)\n",
    "        target = Variable(y.squeeze(1))\n",
    "        testloss = criterion(out, target)\n",
    "        testloss.backward()\n",
    "        optimizer.step()\n",
    "        testloss1 = testloss.data[0]\n",
    "    return np.average(testloss1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainingloss = []\n",
    "trainingloss1 = []\n",
    "trlosses = []\n",
    "tracc = []\n",
    "teacc = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:53: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n",
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:21: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 Training Loss: 1.3969646692276 Training Acc: 55.4054054054054 Testing Loss: 1.7266912460327148 Testing Acc: 49.42857142857143 \n",
      "Train Epoch: 1 Training Loss: 1.3776382207870483 Training Acc: 61.648648648648646 Testing Loss: 1.529322624206543 Testing Acc: 56.285714285714285 \n",
      "Train Epoch: 2 Training Loss: 1.3817651271820068 Training Acc: 52.67567567567568 Testing Loss: 1.557242751121521 Testing Acc: 51.0 \n",
      "Train Epoch: 3 Training Loss: 1.3687045574188232 Training Acc: 58.67567567567568 Testing Loss: 1.639460563659668 Testing Acc: 54.857142857142854 \n",
      "Train Epoch: 4 Training Loss: 1.3547037839889526 Training Acc: 63.945945945945944 Testing Loss: 1.5130494832992554 Testing Acc: 59.57142857142857 \n",
      "Train Epoch: 5 Training Loss: 1.341249704360962 Training Acc: 64.5945945945946 Testing Loss: 1.3404048681259155 Testing Acc: 57.57142857142857 \n",
      "Train Epoch: 6 Training Loss: 1.3302066326141357 Training Acc: 65.29729729729729 Testing Loss: 1.3306587934494019 Testing Acc: 60.857142857142854 \n",
      "Train Epoch: 7 Training Loss: 1.3207722902297974 Training Acc: 64.21621621621621 Testing Loss: 1.257635235786438 Testing Acc: 57.714285714285715 \n",
      "Train Epoch: 8 Training Loss: 1.3139408826828003 Training Acc: 63.62162162162162 Testing Loss: 1.2827471494674683 Testing Acc: 59.0 \n",
      "Train Epoch: 9 Training Loss: 1.3073954582214355 Training Acc: 67.64864864864865 Testing Loss: 1.4717190265655518 Testing Acc: 61.285714285714285 \n",
      "Train Epoch: 10 Training Loss: 1.2996267080307007 Training Acc: 69.75675675675676 Testing Loss: 1.2370948791503906 Testing Acc: 62.57142857142857 \n",
      "Train Epoch: 11 Training Loss: 1.2934669256210327 Training Acc: 69.83783783783784 Testing Loss: 1.3516722917556763 Testing Acc: 64.0 \n",
      "Train Epoch: 12 Training Loss: 1.2890899181365967 Training Acc: 70.35135135135135 Testing Loss: 1.3613404035568237 Testing Acc: 65.42857142857143 \n",
      "Train Epoch: 13 Training Loss: 1.2841017246246338 Training Acc: 70.64864864864865 Testing Loss: 1.3279845714569092 Testing Acc: 64.85714285714286 \n",
      "Train Epoch: 14 Training Loss: 1.2782906293869019 Training Acc: 72.10810810810811 Testing Loss: 1.2572218179702759 Testing Acc: 64.57142857142857 \n",
      "Train Epoch: 15 Training Loss: 1.2724465131759644 Training Acc: 72.8108108108108 Testing Loss: 1.2645519971847534 Testing Acc: 66.85714285714286 \n",
      "Train Epoch: 16 Training Loss: 1.267715334892273 Training Acc: 72.08108108108108 Testing Loss: 1.319057822227478 Testing Acc: 66.0 \n",
      "Train Epoch: 17 Training Loss: 1.2639003992080688 Training Acc: 72.13513513513513 Testing Loss: 1.3499418497085571 Testing Acc: 68.0 \n",
      "Train Epoch: 18 Training Loss: 1.2593542337417603 Training Acc: 74.05405405405405 Testing Loss: 1.060648798942566 Testing Acc: 68.85714285714286 \n",
      "Train Epoch: 19 Training Loss: 1.2543889284133911 Training Acc: 76.29729729729729 Testing Loss: 1.2352346181869507 Testing Acc: 69.14285714285714 \n",
      "Train Epoch: 20 Training Loss: 1.2501177787780762 Training Acc: 74.89189189189189 Testing Loss: 1.1407853364944458 Testing Acc: 69.57142857142857 \n",
      "Train Epoch: 21 Training Loss: 1.2462061643600464 Training Acc: 75.43243243243244 Testing Loss: 1.1938706636428833 Testing Acc: 66.0 \n",
      "Train Epoch: 22 Training Loss: 1.241614580154419 Training Acc: 76.54054054054055 Testing Loss: 1.1839699745178223 Testing Acc: 68.0 \n",
      "Train Epoch: 23 Training Loss: 1.237097978591919 Training Acc: 77.08108108108108 Testing Loss: 1.2817864418029785 Testing Acc: 70.42857142857143 \n",
      "Train Epoch: 24 Training Loss: 1.232856273651123 Training Acc: 78.8108108108108 Testing Loss: 1.2191178798675537 Testing Acc: 72.0 \n",
      "Train Epoch: 25 Training Loss: 1.229062557220459 Training Acc: 78.1891891891892 Testing Loss: 1.3857221603393555 Testing Acc: 71.42857142857143 \n",
      "Train Epoch: 26 Training Loss: 1.2252721786499023 Training Acc: 78.8108108108108 Testing Loss: 1.2127503156661987 Testing Acc: 73.57142857142857 \n",
      "Train Epoch: 27 Training Loss: 1.2222368717193604 Training Acc: 79.89189189189189 Testing Loss: 1.2000185251235962 Testing Acc: 72.71428571428571 \n",
      "Train Epoch: 28 Training Loss: 1.2191543579101562 Training Acc: 79.10810810810811 Testing Loss: 1.093967318534851 Testing Acc: 71.57142857142857 \n",
      "Train Epoch: 29 Training Loss: 1.2161438465118408 Training Acc: 79.56756756756756 Testing Loss: 1.2182272672653198 Testing Acc: 73.85714285714286 \n",
      "Train Epoch: 30 Training Loss: 1.2126356363296509 Training Acc: 79.10810810810811 Testing Loss: 1.1317031383514404 Testing Acc: 73.71428571428571 \n",
      "Train Epoch: 31 Training Loss: 1.20928955078125 Training Acc: 80.97297297297297 Testing Loss: 1.2606983184814453 Testing Acc: 75.42857142857143 \n",
      "Train Epoch: 32 Training Loss: 1.2058982849121094 Training Acc: 81.67567567567568 Testing Loss: 1.1294481754302979 Testing Acc: 74.28571428571429 \n",
      "Train Epoch: 33 Training Loss: 1.2023823261260986 Training Acc: 83.08108108108108 Testing Loss: 1.1874628067016602 Testing Acc: 76.57142857142857 \n",
      "Train Epoch: 34 Training Loss: 1.19907546043396 Training Acc: 83.37837837837837 Testing Loss: 1.0640153884887695 Testing Acc: 77.42857142857143 \n",
      "Train Epoch: 35 Training Loss: 1.195695400238037 Training Acc: 82.45945945945945 Testing Loss: 1.0090094804763794 Testing Acc: 76.71428571428571 \n",
      "Train Epoch: 36 Training Loss: 1.1925452947616577 Training Acc: 83.10810810810811 Testing Loss: 1.110404372215271 Testing Acc: 77.71428571428571 \n",
      "Train Epoch: 37 Training Loss: 1.18971586227417 Training Acc: 83.56756756756756 Testing Loss: 1.0252536535263062 Testing Acc: 77.85714285714286 \n",
      "Train Epoch: 38 Training Loss: 1.186825156211853 Training Acc: 84.02702702702703 Testing Loss: 1.1247444152832031 Testing Acc: 78.57142857142857 \n",
      "Train Epoch: 39 Training Loss: 1.1838098764419556 Training Acc: 84.16216216216216 Testing Loss: 1.1630454063415527 Testing Acc: 78.85714285714286 \n",
      "Train Epoch: 40 Training Loss: 1.1812949180603027 Training Acc: 84.97297297297297 Testing Loss: 1.0121585130691528 Testing Acc: 81.14285714285714 \n",
      "Train Epoch: 41 Training Loss: 1.178226351737976 Training Acc: 84.24324324324324 Testing Loss: 0.9863921403884888 Testing Acc: 78.85714285714286 \n",
      "Train Epoch: 42 Training Loss: 1.1751062870025635 Training Acc: 86.70270270270271 Testing Loss: 0.9829707741737366 Testing Acc: 80.28571428571429 \n",
      "Train Epoch: 43 Training Loss: 1.172163963317871 Training Acc: 86.72972972972973 Testing Loss: 1.0502605438232422 Testing Acc: 81.42857142857143 \n",
      "Train Epoch: 44 Training Loss: 1.1691350936889648 Training Acc: 85.13513513513513 Testing Loss: 1.0652495622634888 Testing Acc: 79.42857142857143 \n",
      "Train Epoch: 45 Training Loss: 1.1663763523101807 Training Acc: 85.86486486486487 Testing Loss: 1.0161712169647217 Testing Acc: 81.0 \n",
      "Train Epoch: 46 Training Loss: 1.1637122631072998 Training Acc: 88.08108108108108 Testing Loss: 1.0343923568725586 Testing Acc: 81.71428571428571 \n",
      "Train Epoch: 47 Training Loss: 1.1609907150268555 Training Acc: 88.21621621621621 Testing Loss: 1.050545334815979 Testing Acc: 83.14285714285714 \n",
      "Train Epoch: 48 Training Loss: 1.158441424369812 Training Acc: 86.70270270270271 Testing Loss: 1.1333028078079224 Testing Acc: 79.42857142857143 \n",
      "Train Epoch: 49 Training Loss: 1.1559187173843384 Training Acc: 87.70270270270271 Testing Loss: 1.0854872465133667 Testing Acc: 83.14285714285714 \n",
      "Train Epoch: 50 Training Loss: 1.1534690856933594 Training Acc: 88.89189189189189 Testing Loss: 0.9672190546989441 Testing Acc: 83.14285714285714 \n",
      "Train Epoch: 51 Training Loss: 1.1508525609970093 Training Acc: 88.16216216216216 Testing Loss: 1.06341552734375 Testing Acc: 83.85714285714286 \n",
      "Train Epoch: 52 Training Loss: 1.148389458656311 Training Acc: 89.37837837837837 Testing Loss: 1.003413200378418 Testing Acc: 85.14285714285714 \n",
      "Train Epoch: 53 Training Loss: 1.145949125289917 Training Acc: 89.48648648648648 Testing Loss: 0.9467041492462158 Testing Acc: 83.85714285714286 \n",
      "Train Epoch: 54 Training Loss: 1.1434262990951538 Training Acc: 89.5945945945946 Testing Loss: 0.987220287322998 Testing Acc: 85.0 \n",
      "Train Epoch: 55 Training Loss: 1.1410006284713745 Training Acc: 88.70270270270271 Testing Loss: 1.1677159070968628 Testing Acc: 82.85714285714286 \n",
      "Train Epoch: 56 Training Loss: 1.138693928718567 Training Acc: 89.02702702702703 Testing Loss: 1.0278637409210205 Testing Acc: 84.14285714285714 \n",
      "Train Epoch: 57 Training Loss: 1.1363842487335205 Training Acc: 90.43243243243244 Testing Loss: 1.0312294960021973 Testing Acc: 84.71428571428571 \n",
      "Train Epoch: 58 Training Loss: 1.134108304977417 Training Acc: 90.27027027027027 Testing Loss: 1.0050108432769775 Testing Acc: 85.14285714285714 \n",
      "Train Epoch: 59 Training Loss: 1.1318923234939575 Training Acc: 90.45945945945945 Testing Loss: 0.9265726804733276 Testing Acc: 84.0 \n",
      "Train Epoch: 60 Training Loss: 1.1297072172164917 Training Acc: 90.48648648648648 Testing Loss: 1.0843088626861572 Testing Acc: 83.28571428571429 \n",
      "Train Epoch: 61 Training Loss: 1.1275606155395508 Training Acc: 91.05405405405405 Testing Loss: 1.0225906372070312 Testing Acc: 85.71428571428571 \n",
      "Train Epoch: 62 Training Loss: 1.1254583597183228 Training Acc: 91.10810810810811 Testing Loss: 0.9930262565612793 Testing Acc: 85.14285714285714 \n",
      "Train Epoch: 63 Training Loss: 1.1234817504882812 Training Acc: 90.67567567567568 Testing Loss: 0.9777794480323792 Testing Acc: 86.0 \n",
      "Train Epoch: 64 Training Loss: 1.121458649635315 Training Acc: 91.16216216216216 Testing Loss: 0.9327225685119629 Testing Acc: 86.0 \n",
      "Train Epoch: 65 Training Loss: 1.1194922924041748 Training Acc: 90.78378378378379 Testing Loss: 0.9862805008888245 Testing Acc: 84.57142857142857 \n",
      "Train Epoch: 66 Training Loss: 1.1176273822784424 Training Acc: 91.51351351351352 Testing Loss: 0.9698541760444641 Testing Acc: 85.42857142857143 \n",
      "Train Epoch: 67 Training Loss: 1.1157201528549194 Training Acc: 91.64864864864865 Testing Loss: 0.9960291981697083 Testing Acc: 86.28571428571429 \n",
      "Train Epoch: 68 Training Loss: 1.1138521432876587 Training Acc: 91.32432432432432 Testing Loss: 0.9253016114234924 Testing Acc: 85.57142857142857 \n",
      "Train Epoch: 69 Training Loss: 1.1120754480361938 Training Acc: 91.67567567567568 Testing Loss: 0.9656795859336853 Testing Acc: 85.85714285714286 \n",
      "Train Epoch: 70 Training Loss: 1.1102640628814697 Training Acc: 91.72972972972973 Testing Loss: 0.9923065304756165 Testing Acc: 86.85714285714286 \n",
      "Train Epoch: 71 Training Loss: 1.1085377931594849 Training Acc: 92.16216216216216 Testing Loss: 0.9864104390144348 Testing Acc: 86.85714285714286 \n",
      "Train Epoch: 72 Training Loss: 1.1067299842834473 Training Acc: 92.24324324324324 Testing Loss: 0.9457504749298096 Testing Acc: 86.57142857142857 \n",
      "Train Epoch: 73 Training Loss: 1.1050429344177246 Training Acc: 92.48648648648648 Testing Loss: 1.0069456100463867 Testing Acc: 87.42857142857143 \n",
      "Train Epoch: 74 Training Loss: 1.1033179759979248 Training Acc: 92.75675675675676 Testing Loss: 0.9657899737358093 Testing Acc: 87.57142857142857 \n",
      "Train Epoch: 75 Training Loss: 1.1016089916229248 Training Acc: 92.64864864864865 Testing Loss: 1.0272678136825562 Testing Acc: 87.0 \n",
      "Train Epoch: 76 Training Loss: 1.0999423265457153 Training Acc: 92.56756756756756 Testing Loss: 0.9653418660163879 Testing Acc: 86.85714285714286 \n",
      "Train Epoch: 77 Training Loss: 1.0983226299285889 Training Acc: 92.86486486486487 Testing Loss: 0.9258771538734436 Testing Acc: 87.57142857142857 \n",
      "Train Epoch: 78 Training Loss: 1.09677255153656 Training Acc: 92.8108108108108 Testing Loss: 0.9221099615097046 Testing Acc: 86.85714285714286 \n",
      "Train Epoch: 79 Training Loss: 1.0952479839324951 Training Acc: 92.78378378378379 Testing Loss: 0.9463632702827454 Testing Acc: 86.14285714285714 \n",
      "Train Epoch: 80 Training Loss: 1.093740701675415 Training Acc: 92.78378378378379 Testing Loss: 0.9301977157592773 Testing Acc: 87.14285714285714 \n",
      "Train Epoch: 81 Training Loss: 1.0922629833221436 Training Acc: 92.83783783783784 Testing Loss: 0.9851982593536377 Testing Acc: 87.28571428571429 \n",
      "Train Epoch: 82 Training Loss: 1.090805172920227 Training Acc: 92.83783783783784 Testing Loss: 0.9252794981002808 Testing Acc: 87.28571428571429 \n",
      "Train Epoch: 83 Training Loss: 1.0894113779067993 Training Acc: 92.72972972972973 Testing Loss: 0.9947201609611511 Testing Acc: 86.14285714285714 \n",
      "Train Epoch: 84 Training Loss: 1.0880287885665894 Training Acc: 92.97297297297297 Testing Loss: 0.9457504749298096 Testing Acc: 87.28571428571429 \n",
      "Train Epoch: 85 Training Loss: 1.086645245552063 Training Acc: 93.1891891891892 Testing Loss: 0.9592257142066956 Testing Acc: 87.14285714285714 \n",
      "Train Epoch: 86 Training Loss: 1.0852817296981812 Training Acc: 92.94594594594595 Testing Loss: 0.9863398671150208 Testing Acc: 87.14285714285714 \n",
      "Train Epoch: 87 Training Loss: 1.0839711427688599 Training Acc: 93.08108108108108 Testing Loss: 0.9611098766326904 Testing Acc: 87.28571428571429 \n",
      "Train Epoch: 88 Training Loss: 1.0827300548553467 Training Acc: 92.21621621621621 Testing Loss: 1.00687837600708 Testing Acc: 86.85714285714286 \n",
      "Train Epoch: 89 Training Loss: 1.0814905166625977 Training Acc: 93.08108108108108 Testing Loss: 0.9899757504463196 Testing Acc: 87.14285714285714 \n",
      "Train Epoch: 90 Training Loss: 1.0802336931228638 Training Acc: 93.16216216216216 Testing Loss: 0.9864599108695984 Testing Acc: 87.14285714285714 \n",
      "Train Epoch: 91 Training Loss: 1.0790166854858398 Training Acc: 92.86486486486487 Testing Loss: 0.9867900013923645 Testing Acc: 87.0 \n",
      "Train Epoch: 92 Training Loss: 1.0778183937072754 Training Acc: 93.29729729729729 Testing Loss: 0.9864672422409058 Testing Acc: 87.28571428571429 \n",
      "Train Epoch: 93 Training Loss: 1.0766751766204834 Training Acc: 92.86486486486487 Testing Loss: 1.0837924480438232 Testing Acc: 87.14285714285714 \n",
      "Train Epoch: 94 Training Loss: 1.0755314826965332 Training Acc: 93.54054054054055 Testing Loss: 0.9456555843353271 Testing Acc: 87.85714285714286 \n",
      "Train Epoch: 95 Training Loss: 1.0743907690048218 Training Acc: 93.5945945945946 Testing Loss: 0.9456573724746704 Testing Acc: 87.28571428571429 \n",
      "Train Epoch: 96 Training Loss: 1.0732393264770508 Training Acc: 93.78378378378379 Testing Loss: 0.9049026370048523 Testing Acc: 87.57142857142857 \n",
      "Train Epoch: 97 Training Loss: 1.0721031427383423 Training Acc: 93.67567567567568 Testing Loss: 0.9862971901893616 Testing Acc: 88.0 \n",
      "Train Epoch: 98 Training Loss: 1.0709847211837769 Training Acc: 93.51351351351352 Testing Loss: 0.924159586429596 Testing Acc: 87.85714285714286 \n",
      "Train Epoch: 99 Training Loss: 1.069893479347229 Training Acc: 93.72972972972973 Testing Loss: 0.955487072467804 Testing Acc: 87.57142857142857 \n",
      "Train Epoch: 100 Training Loss: 1.0688514709472656 Training Acc: 93.62162162162163 Testing Loss: 0.9581372737884521 Testing Acc: 87.85714285714286 \n",
      "Train Epoch: 101 Training Loss: 1.0678545236587524 Training Acc: 93.37837837837837 Testing Loss: 0.9855296015739441 Testing Acc: 87.71428571428571 \n",
      "Train Epoch: 102 Training Loss: 1.0668214559555054 Training Acc: 93.64864864864865 Testing Loss: 0.9449905157089233 Testing Acc: 87.71428571428571 \n",
      "Train Epoch: 103 Training Loss: 1.065809965133667 Training Acc: 93.70270270270271 Testing Loss: 0.9256693124771118 Testing Acc: 87.85714285714286 \n",
      "Train Epoch: 104 Training Loss: 1.064829707145691 Training Acc: 93.78378378378379 Testing Loss: 0.9254052042961121 Testing Acc: 87.85714285714286 \n",
      "Train Epoch: 105 Training Loss: 1.063843011856079 Training Acc: 93.75675675675676 Testing Loss: 1.0026793479919434 Testing Acc: 87.57142857142857 \n",
      "Train Epoch: 106 Training Loss: 1.062889575958252 Training Acc: 93.78378378378379 Testing Loss: 0.9285405278205872 Testing Acc: 87.85714285714286 \n",
      "Train Epoch: 107 Training Loss: 1.0619654655456543 Training Acc: 93.89189189189189 Testing Loss: 0.9448540210723877 Testing Acc: 87.71428571428571 \n",
      "Train Epoch: 108 Training Loss: 1.0610395669937134 Training Acc: 93.75675675675676 Testing Loss: 0.980303168296814 Testing Acc: 87.71428571428571 \n",
      "Train Epoch: 109 Training Loss: 1.0601216554641724 Training Acc: 93.70270270270271 Testing Loss: 0.9456499814987183 Testing Acc: 87.57142857142857 \n",
      "Train Epoch: 110 Training Loss: 1.0592550039291382 Training Acc: 93.94594594594595 Testing Loss: 0.9255602955818176 Testing Acc: 88.0 \n",
      "Train Epoch: 111 Training Loss: 1.0583802461624146 Training Acc: 93.78378378378379 Testing Loss: 0.9737146496772766 Testing Acc: 87.85714285714286 \n",
      "Train Epoch: 112 Training Loss: 1.057533621788025 Training Acc: 94.02702702702703 Testing Loss: 0.9660614132881165 Testing Acc: 87.71428571428571 \n",
      "Train Epoch: 113 Training Loss: 1.0566617250442505 Training Acc: 93.91891891891892 Testing Loss: 0.9430489540100098 Testing Acc: 87.28571428571429 \n",
      "Train Epoch: 114 Training Loss: 1.0558511018753052 Training Acc: 93.94594594594595 Testing Loss: 0.9862540364265442 Testing Acc: 87.42857142857143 \n",
      "Train Epoch: 115 Training Loss: 1.0550179481506348 Training Acc: 94.16216216216216 Testing Loss: 0.9523210525512695 Testing Acc: 88.0 \n",
      "Train Epoch: 116 Training Loss: 1.0542043447494507 Training Acc: 94.02702702702703 Testing Loss: 1.0101631879806519 Testing Acc: 88.0 \n",
      "Train Epoch: 117 Training Loss: 1.0533926486968994 Training Acc: 94.08108108108108 Testing Loss: 1.0019158124923706 Testing Acc: 87.71428571428571 \n",
      "Train Epoch: 118 Training Loss: 1.052607774734497 Training Acc: 93.67567567567568 Testing Loss: 0.9257810115814209 Testing Acc: 87.71428571428571 \n",
      "Train Epoch: 119 Training Loss: 1.0518320798873901 Training Acc: 94.02702702702703 Testing Loss: 0.9527048468589783 Testing Acc: 87.42857142857143 \n",
      "Train Epoch: 120 Training Loss: 1.05106782913208 Training Acc: 94.10810810810811 Testing Loss: 0.92552649974823 Testing Acc: 88.28571428571429 \n",
      "Train Epoch: 121 Training Loss: 1.050296425819397 Training Acc: 94.29729729729729 Testing Loss: 0.9252403974533081 Testing Acc: 88.28571428571429 \n",
      "Train Epoch: 122 Training Loss: 1.049519419670105 Training Acc: 94.32432432432432 Testing Loss: 0.9050559401512146 Testing Acc: 88.28571428571429 \n",
      "Train Epoch: 123 Training Loss: 1.0487626791000366 Training Acc: 94.37837837837837 Testing Loss: 0.9664449691772461 Testing Acc: 88.14285714285714 \n",
      "Train Epoch: 124 Training Loss: 1.048014521598816 Training Acc: 94.4054054054054 Testing Loss: 0.9443456530570984 Testing Acc: 88.28571428571429 \n",
      "Train Epoch: 125 Training Loss: 1.0472689867019653 Training Acc: 94.37837837837837 Testing Loss: 0.9456502199172974 Testing Acc: 88.14285714285714 \n",
      "Train Epoch: 126 Training Loss: 1.0465368032455444 Training Acc: 94.43243243243244 Testing Loss: 0.9864596128463745 Testing Acc: 88.28571428571429 \n",
      "Train Epoch: 127 Training Loss: 1.0458166599273682 Training Acc: 94.4054054054054 Testing Loss: 0.9660624265670776 Testing Acc: 88.14285714285714 \n",
      "Train Epoch: 128 Training Loss: 1.0450993776321411 Training Acc: 94.35135135135135 Testing Loss: 0.9455602765083313 Testing Acc: 87.85714285714286 \n",
      "Train Epoch: 129 Training Loss: 1.0443994998931885 Training Acc: 94.43243243243244 Testing Loss: 0.9252452254295349 Testing Acc: 88.28571428571429 \n",
      "Train Epoch: 130 Training Loss: 1.0437108278274536 Training Acc: 94.37837837837837 Testing Loss: 0.9650977253913879 Testing Acc: 88.0 \n",
      "Train Epoch: 131 Training Loss: 1.0430259704589844 Training Acc: 94.48648648648648 Testing Loss: 0.9252420663833618 Testing Acc: 88.28571428571429 \n",
      "Train Epoch: 132 Training Loss: 1.0423520803451538 Training Acc: 94.51351351351352 Testing Loss: 0.9252433776855469 Testing Acc: 88.28571428571429 \n",
      "Train Epoch: 133 Training Loss: 1.041683316230774 Training Acc: 94.56756756756756 Testing Loss: 0.9443804025650024 Testing Acc: 88.0 \n",
      "Train Epoch: 134 Training Loss: 1.0410248041152954 Training Acc: 94.54054054054055 Testing Loss: 0.9864923357963562 Testing Acc: 88.14285714285714 \n",
      "Train Epoch: 135 Training Loss: 1.0403740406036377 Training Acc: 94.54054054054055 Testing Loss: 0.9252444505691528 Testing Acc: 88.28571428571429 \n",
      "Train Epoch: 136 Training Loss: 1.039732813835144 Training Acc: 94.37837837837837 Testing Loss: 0.9456591606140137 Testing Acc: 87.85714285714286 \n",
      "Train Epoch: 137 Training Loss: 1.0391030311584473 Training Acc: 94.48648648648648 Testing Loss: 0.925249457359314 Testing Acc: 88.14285714285714 \n",
      "Train Epoch: 138 Training Loss: 1.0384917259216309 Training Acc: 94.48648648648648 Testing Loss: 0.9660376906394958 Testing Acc: 88.0 \n",
      "Train Epoch: 139 Training Loss: 1.037879467010498 Training Acc: 94.54054054054055 Testing Loss: 0.9864697456359863 Testing Acc: 88.28571428571429 \n",
      "Train Epoch: 140 Training Loss: 1.0372735261917114 Training Acc: 94.51351351351352 Testing Loss: 0.9455015063285828 Testing Acc: 88.28571428571429 \n",
      "Train Epoch: 141 Training Loss: 1.0366768836975098 Training Acc: 94.56756756756756 Testing Loss: 1.0261592864990234 Testing Acc: 88.14285714285714 \n",
      "Train Epoch: 142 Training Loss: 1.036088466644287 Training Acc: 94.56756756756756 Testing Loss: 0.9252393245697021 Testing Acc: 88.14285714285714 \n",
      "Train Epoch: 143 Training Loss: 1.0355051755905151 Training Acc: 94.5945945945946 Testing Loss: 0.9456606507301331 Testing Acc: 88.14285714285714 \n",
      "Train Epoch: 144 Training Loss: 1.0349323749542236 Training Acc: 94.56756756756756 Testing Loss: 0.9657315611839294 Testing Acc: 88.14285714285714 \n",
      "Train Epoch: 145 Training Loss: 1.0343687534332275 Training Acc: 94.56756756756756 Testing Loss: 0.9245778918266296 Testing Acc: 88.28571428571429 \n",
      "Train Epoch: 146 Training Loss: 1.033808946609497 Training Acc: 94.5945945945946 Testing Loss: 0.9456543326377869 Testing Acc: 88.28571428571429 \n",
      "Train Epoch: 147 Training Loss: 1.0332553386688232 Training Acc: 94.56756756756756 Testing Loss: 0.9660555124282837 Testing Acc: 88.28571428571429 \n",
      "Train Epoch: 148 Training Loss: 1.0327157974243164 Training Acc: 94.62162162162163 Testing Loss: 0.966056764125824 Testing Acc: 88.14285714285714 \n",
      "Train Epoch: 149 Training Loss: 1.0321784019470215 Training Acc: 94.62162162162163 Testing Loss: 0.945650041103363 Testing Acc: 88.28571428571429 \n"
     ]
    }
   ],
   "source": [
    "epochs = 150\n",
    "#criterion sets up the loss function; here Cross Entropy Loss is used\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "#Object of the Network created\n",
    "\n",
    "#net = ConvClassifier()\n",
    "net = VGG('VGG13')\n",
    "\n",
    "if use_cuda:\n",
    "    net = net.cuda()\n",
    "\n",
    "print(\"Print initial weights and biases\")\n",
    "print(\"Default initial weights: \")\n",
    "print(net.conv1.weight)\n",
    "print(\"Default initial biases: \")\n",
    "print(net.conv1.bias)\n",
    "\n",
    "#set up optimizer having parameters like epochs, batch size and learning rate\n",
    "optimizer = optim.SGD(params=net.parameters(), lr=0.03, momentum=0.9)\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "#Train the network\n",
    "for epoch in range(epochs):\n",
    "    net.train(True)\n",
    "    for i, (x, y) in enumerate(trainloader):\n",
    "        #x = x.to(device, dtype=torch.float32)\n",
    "        x=x.type(torch.FloatTensor)\n",
    "        if use_cuda:\n",
    "            x = x.cuda()\n",
    "            y = y.cuda()\n",
    "        \n",
    "        #reinitialise the optimiser gradient to 0\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        #input to the network is training data\n",
    "        inputs = Variable(x)\n",
    "        #output of the network is trained inputs; net() calls the feedborward network\n",
    "        output = net(inputs)\n",
    "        #targets is the training labels\n",
    "        targets = Variable(y.squeeze(1))\n",
    "        \n",
    "        #training loss calculated using criterion which is the Cross Entropy loss in this case\n",
    "        trloss = criterion(output, targets)\n",
    "        #Backward propoagation occurs\n",
    "        trloss.backward()\n",
    "        #Optimizer used to smoothen the learning and reducing the loss\n",
    "        optimizer.step()\n",
    "        \n",
    "        #getting the training loss\n",
    "       ### trlosses.append(trloss.item())\n",
    "        trloss1=trloss.data[0]\n",
    "        trlosses.append(trloss1)\n",
    "\n",
    "    trainingloss.append(np.average(trloss1))\n",
    "    trainacc = eval(net, trainloader)\n",
    "    tracc.append(trainacc)\n",
    "    testacc = eval(net, testloader)\n",
    "    teacc.append(testacc)\n",
    "    testloss = evaltestloss(net, testloader)\n",
    "    testlosses.append(testloss)\n",
    "    print('Train Epoch: {} Training Loss: {} Training Acc: {} Testing Loss: {} Testing Acc: {} '.format(epoch, np.average(trlosses), trainacc, testloss, testacc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Plotting the metrics\n",
    "fig = plt.figure()\n",
    "#printing training accuracy and testing accuracy\n",
    "plt.subplot(2,1,1)\n",
    "plt.plot(tracc)\n",
    "plt.plot(teacc)\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='lower right')\n",
    "#printing training loss and test loss\n",
    "plt.subplot(2,1,2)\n",
    "plt.plot(trainingloss)\n",
    "plt.plot(testlosses)\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper right')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### WRITE UP\n",
    "\n",
    "\n",
    "## 1. Training and testing accuracy vs epoch plots\n",
    "We see the following training and testing accuracy vs epoch plots:\n",
    "<br>Model 1 - VGG13\n",
    "<br>A complex VGG13 model with ReLU activations and last fully connected layer having softmax activation was used. Cross-entropy loss with SGD optimizer gives the best result. The training and testing accuracy achieved is 94.2616% and 88.285% while, train and test loss is 1.032 and 0.945. This was got after 150 epochs with batch size of 100 and learning rate of 0.03 and momentum of 0.9. Plot for the same is shown below:\n",
    "<br>Accuracy Plot and Loss Plot\n",
    "<br><img src=\"best.png\">\n",
    "<br>\n",
    "<br>Model 2\n",
    "<br>The same model and learning rate but with no memomentum gave a varied loss with remained almost the same; 5.72 and 5.52 for train and test respectively, but accuracy curve was seen from 5.6% to 72% in training data and 5.90% to 66.71% in testing data. This is shown by the plot below:\n",
    "<br>Accuracy Plot and Loss Plot\n",
    "<br><img src=\"middle.png\">\n",
    "\n",
    "<br>Model 3\n",
    "<br>The shallower model, with no momentum, learning rate of 0.03 for 10 epochs only, gives, training and testing accuracy of 47.37% and 48.47% and training and testing loss of 0.13208849728107452 and 0.003028332721441984. Also, the log_softmax activation was used, whhich gives a smootehr plot but becomes stagnant very soon. The plot for the same is shown below:\n",
    "<br>Accuracy Plot and Loss Plot\n",
    "<br><img src=\"shallow.png\">\n",
    "\n",
    "\n",
    "\n",
    "## 2. Representative of the weight and biases\n",
    "Default initial weights: \n",
    "\n",
    "[[[[ 0.1074,  0.0619, -0.0035],\n",
    "   [-0.0505, -0.0592, -0.1107],\n",
    "   [-0.0418, -0.0867, -0.1023]], \n",
    "          \n",
    "   ...,\n",
    "\n",
    "   [[-0.1450, -0.0938,  0.1416],\n",
    "   [-0.1406,  0.0216,  0.0858],\n",
    "   [-0.0718, -0.0470,  0.0697]]]]\n",
    "   \n",
    "Default initial biases:\n",
    "\n",
    "[ 0.1442,  0.1304, -0.1518, ....., 0.0081, -0.0288, -0.1886]\n",
    "<br>The initialization of the weights happens as follows: Weights are initialised within the range [-stdv, stdv] where, stdv = 1. / math.sqrt(self.weight.size(1)) and if the bias is not None, then it is initialised within the same range. \n",
    "<br>It is beneficial to use small value for bias and hence, bias = True is included, but its more commonly used to have 0 biases.\n",
    "\n",
    "\n",
    "## 3. What is used and why\n",
    "4.1 Initial Shape\n",
    "<br>The Flower dataset consists of 4323 records having images of the size (3 * 32 * 32).\n",
    "<br>Preprocessing included normalising the images by the mean and deviation so as to centralise it. Training images were also flipped horizontally, at the same time cropped to get zoomed version.\n",
    "<br>Then, splitting of training data and test data is done. This splitting is done as 85%-15% for training data and testing data respectively. \n",
    "\n",
    "<br>4.2 Transforming or preprocessing the data\n",
    "<br>A normalisation of mean and standard deviation of all the images is found in img_mean and img_std to centralise the data and make it less scarttered. This avoids very large differences in gradients and hence learnt weights which would cause deviation.\n",
    "\n",
    "## 4. About network's shape and layer choices, loss function and activation function\n",
    "### 4.1 Network's shape\n",
    "The data is very complex and it requires alot of preprocessing and hence has many parameters. The network consists of 13 convolutional layers, having ReLU applied to them and batch normalization applied after every layer and max-pooling layer applied every 2 layers. Lastly, there is a Average pooling layer and then softmax classification layer.\n",
    "<br>The shape of the input is (3674, 32, 32, 3) that is 3674 images of (3*32*32) dimensions.\n",
    "<br>This performs the classification into 5 classes of flower species.\n",
    "\n",
    "<br><br>Another, shallower CNN is also modeled having 5 convolutional neural networks with ReLU in between and log_softmax in the end for classification.\n",
    "\n",
    "### 4.2 Layer Choice\n",
    "<br><br>4.2.1 CNN\n",
    "<br>This layer takes each images features and learns from them to change the weights and biases to get a 5 class classification of flowers.\n",
    "<br><br>4.2.2 Fully connected layers\n",
    "<br>This layer takes is used to take all the features and reduce it to lower dimensions.\n",
    "<br><br>4.2.2 SGD Optimizer\n",
    "<br>SGD optimizer to gives the results as we have alot of features and batches are taken into consideration.\n",
    "<br><br>4.2.3 Adam Optimizer\n",
    "<br>This is the optimization algorithm used to handle sparse gradients if they occur during training.\n",
    "<br><br>4.2.4 Learning Rate\n",
    "<br>Slower learning rate gives smoother increase in accuracy over faster learning rate as can be seen in the plot.\n",
    "<br><br>4.2.5 Momentum\n",
    "<br>Momentum is the factor between 0 and 1 that increases the step size to come out of local minimas and reach the global minima. A momentum factor of 0.9 is used here, so as to not stagnate the gradient to lessen the speed and inturn make it faster.\n",
    "<br><br>4.2.6 Batch Normalisation\n",
    "<br>Batch normalisation is performed after each and every CNN layer to normalise the weights and biases so that extremities in ranges is not reached and gradients are within some defined range to prevent divergence of learning.\n",
    "\n",
    "### 4.3 Loss Function:\n",
    "Cross Entropy Loss is used as the loss function as, multi-class classification has to be done. This calculates loss error between real values and predicted values.\n",
    "\n",
    "### 4.4 Activation Function:\n",
    "4.4.1 ReLU\n",
    "<br>Used as activation function in the CNN layer as it is the most widely used because it reduces training time and prevents problem of vanishing gradients. This can be understood by its formula; f(x)=max(0, x) where x is input. So, it sets all negative values in input matrix to 0 while keeping other values as is.\n",
    "<br><br>4.4.2 Softmax Activation\n",
    "<br>Softmax converts the output into probability distribution over 5 classes. The addition of probabilities of the 5 classes equals to 1. The class having the largest output softmax probability is the class the image belongs to.\n",
    "\n",
    "### 4.5 Splitting Dataset\n",
    "The dataset is supposed to be technically divided into a 67%-33%; training and testing data because it is widely performed and most efficient. But, here, the minimum split is used, as 85%-15%.\n",
    "\n",
    "\n",
    "### 4.6 Datapoints that failed\n",
    "From the results shown above following are few samples that are incorrectly classified:\n",
    "\n",
    "<br>index prediction actual\n",
    "<br>36 1 2\n",
    "<br>362 2 3\n",
    "<br>645 1 0\n",
    "<br>3241 0 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
